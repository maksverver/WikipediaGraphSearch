Possible optimization of FindShortestPath()
-------------------------------------------

Currently, FindShortestPath() contains a local variable:

    std::vector<index_t> visited(size, 0);

This set stores the predecessor information of visited vertices. It requires
(V * sizeof index_t) bytes of memory up-front. In the typical case, where a
shortest path can be found by visiting only a small subset of the graph (e.g.,
< 10,000 vertices, or < 1% of the graph), creating this vector accounts for a
large share of the time and memory taken.

We could store the visited vertex size in an std::unordered_map<index_t, index_t>
instead, or even better: absl::flat_hash_map<index_t, index_t>. These data
structures use memory proportional to the size of the set, though accessing
elements is slower because of the hashing involved.

However, these data structures are strictly worse in cases where we need to
expand a large fraction of the graph, because they require much more memory per
element (even absl::flat_hash_map would require about 4 times as much memory).

To optimize the common cases without using too much memory in the worst case,
we could change FindShortestPath() to a template function which can be
instantiated with the two possible data structures.

Then, a hybrid algorithm could be implemented as follows:

  1. Start the search with a hash-table-based implementation.
  2. If visited.size() > V/100, switch to the vector-based-implementation.

The switch could be implemented in two ways:

  1. Copy the contents of the hash table to the vector, and keep the existing
     forward/backward fringes.
  2. Restart the search from the beginning with the new data structure.

Option 1 would be more efficient because it would avoid redoing work. Option 2
would be simpler to implement, and would likely not be much worse in practice,
since it is expected that in cases where the shortest path is not found in the
first phase, the second phase will take much longer than the first phase anyway,
so that duplicating the work of the first phase does not increase total runtime
by too much.

These optimizations are currently not implemented because benchmarks are needed
to see if the performance improvements justify the increased complexity.
Additionally, the ideal threshold for switching (1% in the example above) may
depend on the shape of the graph.


Multiple paths in web frontend
------------------------------

Currently, the web frontend (both the old and the new one) shows only a single
shortest path, while the command line `search` tool can show all paths.

It would be nice to be able to show all paths in the web frontend too (possibly
with pagination, though I don't know if that's necessary, since it seems rare
for a pair of articles to have more than 1000 paths between them).


Various other improvements
--------------------------

  - Python module: improve exception message when the graph is not found.
    Currently it looks like this:

        >>> import wikipath
        >>> r = wikipath.Reader('nonexistent')
        Could not open graph file [nonexistent]
        Traceback (most recent call last):
          File "<stdin>", line 1, in <module>
        TypeError: pybind11::init(): factory function returned nullptr

    which is a bit obscure. This might require refactoring src/reader.cc to
    return an error message instead of printing it to directly to std::cerr,
    or maybe I could just handle the most common failure case (file does not
    exist) upfromt with a call to std::filesystem::exists().

  - Python module: support a way to close Reader/GraphReader/MetadataReader
    instances manually.

    This would also allow implementing the context manager protocol
    (https://peps.python.org/pep-0343/) so graphs can be opened with:

        with wikipath.Reader('filename.graph') as r:
          # use r

    Currently, I rely on reference counting to dispose the objects when they
    are no longer used. However, reference counting is an implementation detail
    of CPython, and other Python implementations (notably, PyPy) delay garbage
    collection, which is problematic when many Reader objects are created e.g.
    in a loop: you'd end up with many memory maps and open SQLite3 databases,
    until you run out of virtual addressing space or file descriptors.

    The reason that this is not an easy change is that if you close the Reader
    instance on the C++ side, then all derived objects are invalidated and
    calling their methods would cause the application to crash! The correct way
    to fix this is to have all derived objects check if the associated
    GraphReader/MetadataReader object is still open, which requires some serious
    refactoring of basically all objects in the Python bindings.

    Fixing this is not very urgent since existing Python scripts only open one
    graph during their lifetime.

    A workaround for the issue is to manually call gc.collect() after you're
    done with a Reader instance, to clean up all unreachable objects.

  - CMake: auto-generate contents of testdata/example.{graph, metadata} from
    example.xml.

    This would require first building the indexer to be able to generate the
    derived files. Not sure it's worth the trouble, since the derived files only
    need to be regenerated when either the graph/metadata file formats or the
    test data changes, and none of these change frequently.

  - Extract wikipedia base url from the XML dump, which contains:

    <siteinfo>
      <base>https://en.wikipedia.org/wiki/Main_Page</base>
    </siteinfo>

    This would require adding a table for site info to the metadata database.
    This could replace the --wiki_base_url argument for the http-server.py.

  - Automatically normalize article titles by replacing underscores with spaces?
    This would allow users to enter "Main_Page" instead of "Main Page".

    Taking this further, maybe we could decode and accept an entire Wikipedia
    URL like https://en.wikipedia.org/wiki/Main_Page?

  - http-server.py: return cache headers.

    Most results can be safely cached either indefinitely or based on the
    modification time of the graph/metadata files, except when requesting a
    random page (i.e. one of the query parameters is '?'), in which case the
    response is always different.
